{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eEmrRL4drKcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt7x7zR8C-ox"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---------------------------\n",
        "# Reproducibility & device\n",
        "# ---------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden=[64, 32], dropout=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = in_dim\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))  # single logit for BCEWithLogits\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)  # [batch]\n",
        "\n",
        "# ---------------------------\n",
        "# Data helpers\n",
        "# ---------------------------\n",
        "def load_data(csv_path: str, target_col: str = \"Outcome\", test_size=0.2, val_size=0.1, random_state=42) -> Tuple:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    assert target_col in df.columns, f\"{target_col} not in CSV\"\n",
        "    X = df.drop(columns=[target_col]).values.astype(np.float32)\n",
        "    y = df[target_col].values.astype(np.int64)\n",
        "\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=test_size+val_size,\n",
        "                                                      random_state=random_state, stratify=y)\n",
        "    rel_val = val_size / (test_size + val_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=rel_val,\n",
        "                                                    random_state=random_state, stratify=y_tmp)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler\n",
        "\n",
        "def make_loader(X, y, batch_size=32, shuffle=True):\n",
        "    tX = torch.from_numpy(X)\n",
        "    ty = torch.from_numpy(y).float()  # BCEWithLogits expects float targets\n",
        "    ds = TensorDataset(tX, ty)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# ---------------------------\n",
        "# Train / Eval\n",
        "# ---------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        logits = model(Xb)  # [batch]\n",
        "        loss = criterion(logits, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, preds, probs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            p = torch.sigmoid(logits).cpu().numpy()\n",
        "            pred = (p >= 0.5).astype(int)\n",
        "            probs.extend(p.tolist())\n",
        "            preds.extend(pred.tolist())\n",
        "            ys.extend(yb.numpy().astype(int).tolist())\n",
        "    acc = accuracy_score(ys, preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(ys, probs)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    return acc, auc\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "def main(args):\n",
        "    set_seed(args.seed)\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test), scaler = load_data(args.csv,\n",
        "                target_col=args.target, test_size=args.test_size, val_size=args.val_size, random_state=args.seed)\n",
        "    train_loader = make_loader(X_train, y_train, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = make_loader(X_val, y_val, batch_size=args.batch_size, shuffle=False)\n",
        "    test_loader = make_loader(X_test, y_test, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    model = MLP(in_dim=X_train.shape[1], hidden=[args.h1, args.h2], dropout=args.dropout).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    best_val_auc = 0.0\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_acc, val_auc = evaluate(model, val_loader)\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} val_acc={val_acc:.4f} val_auc={val_auc:.4f}\")\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            ckpt = os.path.join(args.save_dir, \"diabetes_mlp_best.pt\")\n",
        "            torch.save({\"model_state\": model.state_dict(), \"scaler\": scaler}, ckpt)\n",
        "            print(f\"  Saved best checkpoint to {ckpt}\")\n",
        "\n",
        "    test_acc, test_auc = evaluate(model, test_loader)\n",
        "    print(f\"Test acc={test_acc:.4f} auc={test_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.csv = \"/content/drive/MyDrive/hqud-hs/diabetes.csv\"\n",
        "            self.target = \"Outcome\"\n",
        "            self.epochs = 30\n",
        "            self.batch_size = 32\n",
        "            self.lr = 1e-3\n",
        "            self.h1 = 64\n",
        "            self.h2 = 32\n",
        "            self.dropout = 0.3\n",
        "            self.save_dir = \"checkpoints\"\n",
        "            self.seed = 42\n",
        "            self.test_size = 0.2\n",
        "            self.val_size = 0.1\n",
        "\n",
        "    args = Args()\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "j088IuCcrfFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Biểu đồ trái – Training Loss per Epoch:\n",
        "Đường loss giảm rõ rệt qua các epoch đầu, chứng tỏ mô hình đang học hiệu quả và dần tối ưu các tham số.\n",
        "\n",
        "Sau khoảng epoch thứ 10, loss bắt đầu ổn định quanh giá trị thấp, thể hiện mô hình đã gần hội tụ.\n",
        "\n",
        "Đường cong mượt, ít dao động, cho thấy quá trình huấn luyện diễn ra ổn định và không bị nhiễu dữ liệu.\n",
        "\n",
        "Biểu đồ phải – Validation Accuracy và AUC per Epoch:\n",
        "\n",
        "Độ chính xác (Accuracy) tăng nhanh trong giai đoạn đầu, sau đó dao động nhẹ quanh mức 0.73–0.75, thể hiện khả năng dự đoán ổn định.\n",
        "\n",
        "Chỉ số AUC đạt giá trị cao (khoảng 0.84), phản ánh mô hình có năng lực phân biệt tốt giữa hai nhóm bệnh và không bệnh.\n",
        "\n",
        "AUC duy trì ổn định qua các epoch, cho thấy mô hình không bị overfitting và có khả năng tổng quát hóa tốt"
      ],
      "metadata": {
        "id": "zjsN58d2EH3F"
      }
    }
  ]
}